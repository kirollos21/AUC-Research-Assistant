"""
LLM client service for Mistral integration
"""

import logging
import warnings
from typing import Any, AsyncIterator, Dict, List, Optional, Union, cast

from langchain_core.language_models import BaseChatModel
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.messages.base import BaseMessage, BaseMessageChunk
from langchain_core.output_parsers import PydanticOutputParser
from langchain_mistralai.chat_models import ChatMistralAI
from langchain_openai.chat_models import ChatOpenAI
from langchain_ollama.chat_models import ChatOllama
from pydantic import BaseModel, Field
from app.core.config import settings


# TODO: move to more relevant schema file
class ChatMessage(BaseModel):
    """OpenAI-compatible message format"""

    role: str = Field(..., description="Role of the message (system, user, assistant)")
    content: str = Field(..., description="Content of the message")


logger = logging.getLogger(__name__)


# TODO: remove the focus parameter and refactor app to account for that
class DatabaseQuery(BaseModel):
    """Structure for database queries generated by LLM"""

    query: str = Field(description="Search query for academic database")
    focus: str = Field(description="Main focus or topic of this query")


class DatabaseQueries(BaseModel):
    """Collection of database queries"""

    queries: List[DatabaseQuery] = Field(
        description="List of 1-5 search queries for academic databases",
        min_length=1,
        max_length=5,
    )


class LLMClient:
    """Mistral LLM client for query generation and RAG responses"""

    def __init__(self) -> None:
        self.llm: BaseChatModel
        match settings.LLM_PROVIDER:
            case "mistral":
                if not settings.MISTRAL_API_KEY:
                    raise ValueError("MISTRAL_API_KEY not found in settings")
                self.llm = ChatMistralAI(
                    model=settings.LLM_MODEL,
                    temperature=settings.LLM_TEMPERATURE,
                    max_tokens=settings.LLM_MAX_OUTPUT_TOKENS,
                    max_retries=3,
                )
            case "openai":
                if not settings.OPENAI_API_KEY:
                    raise ValueError("OPENAI_API_KEY not found in settings")
                self.llm = ChatOpenAI(
                    model=settings.LLM_MODEL,
                    temperature=settings.LLM_TEMPERATURE,
                    max_tokens=settings.LLM_MAX_OUTPUT_TOKENS,
                    max_retries=3,
                    base_url=settings.LLM_OPENAI_BASEURL,
                )
            case "ollama":
                self.llm = ChatOllama(
                    model=settings.LLM_MODEL,
                    temperature=settings.LLM_TEMPERATURE,
                    max_tokens=settings.LLM_MAX_OUTPUT_TOKENS,
                    max_retries=3,
                )

        self.query_parser: PydanticOutputParser = PydanticOutputParser(
            pydantic_object=DatabaseQueries
        )

    async def generate_database_queries(self, user_query: str) -> List[Dict[str, str]]:
        """
        Generate 1-5 database search queries from user input using structured output

        Args:
            user_query: The user's research question

        Returns:
            List of dictionaries with 'query' and 'focus' keys
        """
        warnings.warn(
            f"generate_database_queries() in file {__file__} is deprecated and will be removed in a future version. Use generate_database_queries_from_conversation() instead.",
            category=DeprecationWarning,
            stacklevel=2,
        )
        system_prompt = """You are an expert academic research assistant. Your task is to generate 1-5 focused search queries for academic databases based on the user's research question.

Guidelines:
- Generate queries that will find relevant academic papers, articles, and research
- Each query should focus on a different aspect or perspective of the topic
- Use academic terminology and keywords
- Keep queries concise but specific
- Generate between 1-5 queries (more complex topics need more queries)
- Each query should have a clear focus description

{format_instructions}"""

        human_prompt: str = f"""User research question: {user_query}

Generate targeted search queries for academic databases that will help find relevant research papers and articles."""

        messages: List[Union[SystemMessage, HumanMessage]] = [
            SystemMessage(
                content=system_prompt.format(
                    format_instructions=self.query_parser.get_format_instructions()
                )
            ),
            HumanMessage(content=human_prompt),
        ]

        try:
            response = await self.llm.ainvoke(messages)
            parsed_queries: DatabaseQueries = self.query_parser.parse(
                cast(str, response.content)
            )

            return [
                {"query": q.query, "focus": q.focus} for q in parsed_queries.queries
            ]
        except Exception as e:
            # Fallback: return the original query if structured output fails
            logging.exception(e)
            return [{"query": user_query, "focus": "Original query"}]

    async def generate_clarification_request(
        self, user_query: str
    ) -> AsyncIterator[BaseMessageChunk]:
        """
        Generate a clarification request for the user's first message.
        This asks for more details and checks if other languages might be helpful.

        Args:
            user_query: The user's initial research question

        Returns:
            Async iterator of message chunks for streaming response
        """
        system_prompt = """You are an expert academic research assistant. The user has just asked their first research question. Your task is to:

1. Ask for clarifications to make their query more specific and focused
2. Identify aspects that need more detail (time period, geographic scope, specific subtopics, methodology preferences)
3. If the topic can benefit from sources in languages other than the user's query language, then ask them if they would like to fetch sources in that language. For example, if the user is doing research about Islamic history, ask if they would like to fetch sources in Arabic as well as the user's query language. If the query is regarding the Berlin wall, ask if the user would like to fetch sources in German as well as the user's query language. If the query does not realistically benefit from adding sources in another language, like if the user's query language is English and the query is regarding a common technology, then do not ask this question. Only ask it when appropriate.

Be helpful and encouraging. Ask 2-4 specific clarifying questions that will help you provide better research assistance. Keep your response concise but thorough."""

        human_prompt = f"""The user asked: "{user_query}"

Please ask for clarifications to help them get the most relevant academic sources for their research."""

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_prompt),
        ]

        try:
            logger.debug(f"Generating stream for user query: {user_query}")
            return self.llm.astream(messages)
        except Exception as e:
            logger.exception(f"Error generating clarification request: {e}")

            # Fallback to a simple clarification message
            async def fallback_stream():
                fallback_message = "Could you please provide more details about your research question? For example, what specific time period, geographic region, or aspects are you most interested in?"
                for word in fallback_message.split():
                    chunk = type("MockChunk", (), {"content": word + " "})()
                    yield chunk

            # TODO: Fix apparent type error
            return fallback_stream()

    # TODO: improve typing
    async def generate_database_queries_from_conversation(
        self, conversation_history: List[ChatMessage]
    ) -> List[Dict[str, str]]:
        """
        Generate database search queries from the entire conversation history,
        but focus on the latest user message for the actual search.

        Args:
            conversation_history: List of conversation messages

        Returns:
            List of dictionaries with 'query' and 'focus' keys
        """
        # Convert conversation to LangChain messages and find latest user query
        langchain_messages: List[BaseMessage] = []
        latest_user_query = ""

        # Add system message first
        system_prompt = """You are an expert academic research assistant. Your task is to generate 1-5 focused search queries for academic databases based on the conversation context and the user's latest research question.

Guidelines:
- Focus your search queries on the LATEST user message - that's what they want documents for now
- Use the conversation history to understand context and refinements
- Generate queries that will find relevant academic papers, articles, and research
- Each query should focus on a different aspect or perspective of the latest topic
- Use academic terminology and keywords
- Keep queries concise but specific
- Generate between 1-5 queries (more complex topics need more queries)
- Each query should have a clear focus description
- Generate the queries in natural language, without database specicific special keywords like AND and OR and using brackets

{format_instructions}"""

        langchain_messages.append(
            SystemMessage(
                content=system_prompt.format(
                    format_instructions=self.query_parser.get_format_instructions()
                )
            )
        )

        # Convert conversation messages to LangChain format
        for msg in conversation_history:
            role = getattr(msg, "role", "")
            content = getattr(msg, "content", "")

            if role == "user":
                langchain_messages.append(HumanMessage(content=content))
                latest_user_query = content  # Keep updating to get the latest
            elif role == "assistant":
                langchain_messages.append(AIMessage(content=content))

        # Add final instruction
        langchain_messages[-1].content = (
            cast(str, langchain_messages[-1].content)
            + "\n\nREMINDER TO AI ASSISTANT: Generate targeted search queries for academic databases as outlined in the system message that will help find relevant research papers and articles for my latest question."
        )

        try:
            response = await self.llm.ainvoke(langchain_messages)
            parsed_queries: DatabaseQueries = self.query_parser.parse(
                cast(str, response.content)
            )

            return [
                {"query": q.query, "focus": q.focus} for q in parsed_queries.queries
            ]
        except Exception as e:
            # Fallback: return the latest query if structured output fails
            logging.exception(e)
            return [{"query": latest_user_query, "focus": "Latest query"}]

    async def generate_rag_response(
        self,
        user_query: str,
        context_documents: List[Dict[str, Any]],
    ) -> AsyncIterator[BaseMessageChunk]:
        """
        Generate response using retrieved documents as context

        Args:
            user_query: The user's original question
            context_documents: List of relevant documents from vector search

        Returns:
            Async iterator that generates message chunks that can be collected into a message
        """
        warnings.warn(
            f"generate_rag_response() in file {__file__} is deprecated and will be removed in a future version. Use generate_rag_response_from_conversation() instead.",
            category=DeprecationWarning,
            stacklevel=2,
        )
        # Format context documents
        context_text: str = ""
        for i, doc in enumerate(context_documents, 1):
            title: str = doc.get("title", "Unknown Title")
            abstract: str = doc.get("abstract", "No abstract available")
            authors: List[str] = doc.get("authors", [])
            authors_str: str = ", ".join(authors) if authors else "Unknown Authors"

            context_text += f"\n--- Document {i} ---\n"
            context_text += f"Title: {title}\n"
            context_text += f"Authors: {authors_str}\n"
            context_text += f"Abstract: {abstract}\n"

        system_prompt: str = """You are an expert academic research assistant. Your task is to answer the user's research question based on the provided academic documents.

Guidelines:
- Use only the information from the provided documents
- Cite specific papers when making claims (use the document titles)
- If the documents don't contain enough information to answer the question, say so
- Provide a comprehensive answer that synthesizes information from multiple sources
- Maintain academic tone and accuracy
- If you find conflicting information, acknowledge it and explain the different perspectives"""

        human_prompt: str = f"""User question: {user_query}

Relevant academic documents:
{context_text}

Please provide a comprehensive answer to the user's question based on the academic documents provided above."""

        messages: List[Union[SystemMessage, HumanMessage]] = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_prompt),
        ]

        return self.llm.astream(messages)

    # TODO: maybe pass the entire document along if possible and not just the title, authors, and abstract?
    async def generate_rag_response_from_conversation(
        self,
        conversation_history: List[ChatMessage],
        context_documents: List[Dict[str, Any]],
    ) -> AsyncIterator[BaseMessageChunk]:
        """
        Generate response using retrieved documents as context and the entire conversation history.
        Preserves the natural conversation flow instead of flattening to text.

        Args:
            conversation_history: List of conversation messages
            context_documents: List of relevant documents from vector search

        Returns:
            Async iterator that generates message chunks for streaming response
        """
        # Format context documents
        context_text: str = ""
        for i, doc in enumerate(context_documents, 1):
            title: str = doc.get("title", "Unknown Title")
            abstract: str = doc.get("abstract", "No abstract available")
            authors: List[str] = doc.get("authors", [])
            authors_str: str = ", ".join(authors) if authors else "Unknown Authors"

            context_text += f"\n--- Document {i} ---\n"
            context_text += f"Title: {title}\n"
            context_text += f"Authors: {authors_str}\n"
            context_text += f"Abstract: {abstract}\n\n"

        # Create system message with context and instructions
        system_prompt = f"""You are an expert academic research assistant. Your task is to answer the user's research question based on the provided academic documents and conversation context.

Guidelines:
- Use only the information from the provided documents
- Consider the conversation history to understand context and any clarifications
- Focus your answer on the user's LATEST question while being informed by the conversation context
- Cite specific papers when making claims (use document index for citation with IEEE style, like [1] or [2-5])
- If the documents don't contain enough information to answer the question, say so
- Provide a comprehensive answer that synthesizes information from multiple sources
- Maintain academic tone and accuracy
- If you find conflicting information, acknowledge it and explain the different perspectives

Relevant academic documents:
{context_text}"""

        # Build message list with system prompt first, then conversation history
        messages: List[BaseMessage] = [SystemMessage(content=system_prompt)]

        # Add the conversation history as-is to preserve natural flow
        for msg in conversation_history:
            role = getattr(msg, "role", "")
            content = getattr(msg, "content", "")

            if role == "user":
                messages.append(HumanMessage(content=content))
            elif role == "assistant":
                messages.append(AIMessage(content=content))
            # Skip system messages from conversation as we already have our system prompt

        return self.llm.astream(messages)


# Global instance
_llm_client: Optional[LLMClient] = None


def get_llm_client() -> LLMClient:
    """Get or create LLM client instance"""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client
