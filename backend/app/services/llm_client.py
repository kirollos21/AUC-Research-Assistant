"""
LLM client service for Mistral/OpenAI integration
"""

from typing import AsyncIterator, List, Dict, Any, Optional, Union
import logging

from pydantic import BaseModel, Field

from langchain_core.language_models import BaseChatModel
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.messages.base import BaseMessageChunk
from langchain_core.output_parsers import PydanticOutputParser

from langchain_mistralai.chat_models import ChatMistralAI
from langchain_openai import ChatOpenAI  # 0.1.x export

from app.core.config import settings

logger = logging.getLogger(__name__)


class DatabaseQuery(BaseModel):
    """Structure for database queries generated by LLM"""
    query: str = Field(description="Search query for academic database")
    focus: str = Field(description="Main focus or topic of this query")


class DatabaseQueries(BaseModel):
    """Collection of database queries"""
    queries: List[DatabaseQuery] = Field(
        description="List of 1-5 search queries for academic databases",
        min_length=1,
        max_length=5,
    )


class LLMClient:
    """LLM client for query generation and RAG responses"""

    def __init__(self) -> None:
        provider = (settings.LLM_PROVIDER or "mistral").lower()
        model = settings.LLM_MODEL or "open-mixtral-8x7b"

        if provider == "mistral":
            if not settings.MISTRAL_API_KEY:
                raise ValueError("MISTRAL_API_KEY not found in settings")
            self.llm: BaseChatModel = ChatMistralAI(
                api_key=settings.MISTRAL_API_KEY,
                model=model,
                temperature=settings.LLM_TEMPERATURE,
                max_tokens=settings.LLM_MAX_OUTPUT_TOKENS,
                max_retries=3,
            )
        elif provider == "openai":
            if not settings.OPENAI_API_KEY:
                raise ValueError("OPENAI_API_KEY not found in settings")
            self.llm: BaseChatModel = ChatOpenAI(
                api_key=settings.OPENAI_API_KEY,
                model=model,
                temperature=settings.LLM_TEMPERATURE,
                max_tokens=settings.LLM_MAX_OUTPUT_TOKENS,
                max_retries=3,
                base_url=getattr(settings, "LLM_OPENAI_BASEURL", None),
            )
        else:
            raise RuntimeError(f"Unsupported LLM_PROVIDER: {provider}")

        # must be inside __init__
        self.query_parser: PydanticOutputParser = PydanticOutputParser(
            pydantic_object=DatabaseQueries
        )
        logger.info(f"LLM configured → provider={provider}, model={model}")

    async def generate_database_queries(self, user_query: str) -> List[Dict[str, str]]:
        """
        Generate 1–5 database search queries from user input using structured output.
        """
        system_prompt = """You are an expert academic research assistant. Your task is to generate 1-5 focused search queries for academic databases based on the user's research question.

Guidelines:
- Generate queries that will find relevant academic papers, articles, and research
- Each query should focus on a different aspect or perspective of the topic
- Use academic terminology and keywords
- Keep queries concise but specific
- Generate between 1-5 queries (more complex topics need more queries)
- Each query should have a clear focus description

{format_instructions}"""

        human_prompt: str = f"""User research question: {user_query}

Generate targeted search queries for academic databases that will help find relevant research papers and articles."""

        messages: List[Union[SystemMessage, HumanMessage]] = [
            SystemMessage(
                content=system_prompt.format(
                    format_instructions=self.query_parser.get_format_instructions()
                )
            ),
            HumanMessage(content=human_prompt),
        ]

        try:
            response = await self.llm.ainvoke(messages)
            parsed: DatabaseQueries = self.query_parser.parse(response.content)
            return [{"query": q.query, "focus": q.focus} for q in parsed.queries]
        except Exception as e:
            logger.exception("Query generation failed; falling back to original query: %s", e)
            return [{"query": user_query, "focus": "Original query"}]

    async def generate_rag_response(
        self,
        user_query: str,
        context_documents: List[Dict[str, Any]],
    ) -> AsyncIterator[BaseMessageChunk]:
        """
        Stream a response using retrieved documents as context.
        """
        # Format context documents
        parts: List[str] = []
        for i, doc in enumerate(context_documents, 1):
            title = doc.get("title", "Unknown Title")
            abstract = doc.get("abstract", "No abstract available")
            authors = doc.get("authors", [])
            authors_str = ", ".join(authors) if authors else "Unknown Authors"
            parts.append(
                f"\n--- Document {i} ---\nTitle: {title}\nAuthors: {authors_str}\nAbstract: {abstract}\n"
            )
        context_text = "".join(parts)

        system_prompt = """You are an expert academic research assistant. Your task is to answer the user's research question based on the provided academic documents.

Guidelines:
- Use only the information from the provided documents
- Cite specific papers when making claims (use the document titles)
- If the documents don't contain enough information to answer the question, say so
- Provide a comprehensive answer that synthesizes information from multiple sources
- Maintain academic tone and accuracy
- If you find conflicting information, acknowledge it and explain the different perspectives"""

        human_prompt = f"""User question: {user_query}

Relevant academic documents:
{context_text}

Please provide a comprehensive answer to the user's question based on the academic documents provided above."""

        messages: List[Union[SystemMessage, HumanMessage]] = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_prompt),
        ]

        return self.llm.astream(messages)


# Global instance
_llm_client: Optional[LLMClient] = None


def get_llm_client() -> LLMClient:
    """Get or create LLM client instance"""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client
